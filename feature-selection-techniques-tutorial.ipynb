{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature Selection Techniques tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "This is an introductory tutorial on some famous feature selection techniques. A large portion of the techniques are studied from\n",
    "'Feature Engineering made easy' by Sinan Ozdemir and Divya Susarla. You can use this tutorial as a support while reading from the book and feel free to play with the code. I have used a famous dataset 'Titanic' since it is pretty easy to understand, and our main goal is to predict who will survive ('Survived' column).A brief description for each feature selection technique is provided, in addition to its pros and cons. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "0. [Data Preparation](#0)\n",
    "1. [What is feature selection](#1)\n",
    "2. [Measuring the effect of feature selection on machine learning performance](#2)\n",
    "3. [Feature Selection Techniques](#3)\n",
    "     - [Univariate Statistics](#4)\n",
    "          * [Selecting features using Pearson's Correlation](#4.1)   \n",
    "          * [Hypthesis Testing using selectKBest](#4.2) \n",
    "     - [Model-Based Feature Selection](#5)\n",
    "          * [Decision Trees](#5.1)\n",
    "          * [Logistic Regression](#5.2)\n",
    "          * [LinearSVM](#5.3)\n",
    "4. [References](#6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n",
      "/kaggle/input/titanic/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# Reading data from csv files:\n",
    "train= pd.read_csv('../input/titanic/train.csv',nrows=100000)\n",
    "test = pd.read_csv('../input/titanic/test.csv',nrows=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data Preparation <a id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Some columns will be removed in this dataset. The tutorial focuses on implementing feature selection techniques, so we can deal with the removed features ( 'Cabin' and 'Ticket' ) in another version for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived      int64\n",
      "Pclass        int64\n",
      "Sex          object\n",
      "Age         float64\n",
      "SibSp         int64\n",
      "Parch         int64\n",
      "Fare        float64\n",
      "Embarked     object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0           0       3    male  22.0      1      0   7.2500        S\n",
       "1           1       1  female  38.0      1      0  71.2833        C\n",
       "2           1       3  female  26.0      0      0   7.9250        S\n",
       "3           1       1  female  35.0      1      0  53.1000        S\n",
       "4           0       3    male  35.0      0      0   8.0500        S\n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...\n",
       "886         0       2    male  27.0      0      0  13.0000        S\n",
       "887         1       1  female  19.0      0      0  30.0000        S\n",
       "888         0       3  female   NaN      1      2  23.4500        S\n",
       "889         1       1    male  26.0      0      0  30.0000        C\n",
       "890         0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= train.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\n",
    "print(train.dtypes)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "For the machine learning models to understand the data, we need to change labels into numerical categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      577\n",
       "female    314\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_nums = { \"Embarked\": {\"S\": 0, \"C\": 1, \"Q\": 2 },\"Sex\":     {\"male\": 0, \"female\": 1}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "0         0       3    0  22.0      1      0   7.2500       0.0\n",
       "1         1       1    1  38.0      1      0  71.2833       1.0\n",
       "2         1       3    1  26.0      0      0   7.9250       0.0\n",
       "3         1       1    1  35.0      1      0  53.1000       0.0\n",
       "4         0       3    0  35.0      0      0   8.0500       0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.replace(cleanup_nums, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Rows with null values will be removed to avoid errors while using the dataset in the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "0           0       3    0  22.0      1      0   7.2500       0.0\n",
       "1           1       1    1  38.0      1      0  71.2833       1.0\n",
       "2           1       3    1  26.0      0      0   7.9250       0.0\n",
       "3           1       1    1  35.0      1      0  53.1000       0.0\n",
       "4           0       3    0  35.0      0      0   8.0500       0.0\n",
       "..        ...     ...  ...   ...    ...    ...      ...       ...\n",
       "885         0       3    1  39.0      0      5  29.1250       2.0\n",
       "886         0       2    0  27.0      0      0  13.0000       0.0\n",
       "887         1       1    1  19.0      0      0  30.0000       0.0\n",
       "889         1       1    0  26.0      0      0  30.0000       1.0\n",
       "890         0       3    0  32.0      0      0   7.7500       2.0\n",
       "\n",
       "[712 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.dropna()\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# What is feature selection<a id=\"1\"></a>\n",
    "\n",
    "Feature selection is a subset of feature engineering and it aims at excluding features that are not important, leaving only the better features i.e the features that are the best when it comes to model prediction. \n",
    "***\n",
    "# **Advantages of feature selection:**\n",
    "1. results in a better performing model.\n",
    "2. creates an easier to understand model. \n",
    "3. results in a model that runs faster. \n",
    "4. reduces the chance of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Measuring the effect of feature selection on machine learning performance<a id=\"2\"></a>\n",
    "We need a function to evaluate the performance of each feature selection technique we use in this tutorial. To optimise the performance of our machine learning model, we will tune its hyperparameters using gridsearch. Check this https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html to read about gridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best_model_and_accuracy(model, params, X, y):\n",
    "    grid = GridSearchCV(model,params,error_score=0., verbose=0, n_jobs=2)\n",
    "    grid.fit(X, y) # fit the model and parameters\n",
    "    s= \"Best Accuracy: {}\".format(grid.best_score_)+ '\\n'+\\\n",
    "       \"Best Parameters: {}\".format(grid.best_params_) +'\\n'+\\\n",
    "       \"Average Time to Fit (s):{}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3)) +'\\n'+\\\n",
    "       \"Average Time to Score (s):{}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for training set: \n",
      " Best Accuracy: 0.7992416034669556\n",
      "Best Parameters: {'max_depth': 5}\n",
      "Average Time to Fit (s):0.005\n",
      "Average Time to Score (s):0.003\n"
     ]
    }
   ],
   "source": [
    "# Next we need to separate all columns from the target column to allow our models to make predictions. \n",
    "X = train.drop('Survived', axis=1)\n",
    "# create our response variable\n",
    "y = train['Survived']\n",
    "# here we will write our machine learning paramters \n",
    "tree_params = {'max_depth':[None,1, 3, 5,7]}\n",
    "# decision tree is the classifier thta we will use. \n",
    "d_tree = DecisionTreeClassifier()\n",
    "\n",
    "print('Results for training set:','\\n',get_best_model_and_accuracy(d_tree,tree_params,X, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature Selection Techniques <a id=\"3\"></a>\n",
    "We will explore three main general techniques and illustrate each one by implementing code on the given dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1- Univariate Statistics <a id=\"4\"></a>\n",
    "\n",
    "Univariate statistics involves checking for statistically significant relationship between each feature and the target. We will focus on two techniques under this heading: Pearson's Correlation and Hypothesis Testing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Selecting features using Pearson's Correlation <a id=\"4.1\"></a>\n",
    "Pearson correlation is a statistic that measures linear correlation between two variables X and Y. It has a value between +1 and âˆ’1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.356462</td>\n",
       "      <td>0.536762</td>\n",
       "      <td>-0.082446</td>\n",
       "      <td>-0.015523</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.108517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.356462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.150826</td>\n",
       "      <td>-0.365902</td>\n",
       "      <td>0.065187</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>-0.552893</td>\n",
       "      <td>-0.108502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0.536762</td>\n",
       "      <td>-0.150826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.099037</td>\n",
       "      <td>0.106296</td>\n",
       "      <td>0.249543</td>\n",
       "      <td>0.182457</td>\n",
       "      <td>0.097129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.082446</td>\n",
       "      <td>-0.365902</td>\n",
       "      <td>-0.099037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.307351</td>\n",
       "      <td>-0.187896</td>\n",
       "      <td>0.093143</td>\n",
       "      <td>0.012186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.015523</td>\n",
       "      <td>0.065187</td>\n",
       "      <td>0.106296</td>\n",
       "      <td>-0.307351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.383338</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>0.004021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.095265</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>0.249543</td>\n",
       "      <td>-0.187896</td>\n",
       "      <td>0.383338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.206624</td>\n",
       "      <td>-0.014082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.266100</td>\n",
       "      <td>-0.552893</td>\n",
       "      <td>0.182457</td>\n",
       "      <td>0.093143</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>0.206624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0.108517</td>\n",
       "      <td>-0.108502</td>\n",
       "      <td>0.097129</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>0.176859</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Survived    Pclass       Sex       Age     SibSp     Parch  \\\n",
       "Survived  1.000000 -0.356462  0.536762 -0.082446 -0.015523  0.095265   \n",
       "Pclass   -0.356462  1.000000 -0.150826 -0.365902  0.065187  0.023666   \n",
       "Sex       0.536762 -0.150826  1.000000 -0.099037  0.106296  0.249543   \n",
       "Age      -0.082446 -0.365902 -0.099037  1.000000 -0.307351 -0.187896   \n",
       "SibSp    -0.015523  0.065187  0.106296 -0.307351  1.000000  0.383338   \n",
       "Parch     0.095265  0.023666  0.249543 -0.187896  0.383338  1.000000   \n",
       "Fare      0.266100 -0.552893  0.182457  0.093143  0.139860  0.206624   \n",
       "Embarked  0.108517 -0.108502  0.097129  0.012186  0.004021 -0.014082   \n",
       "\n",
       "              Fare  Embarked  \n",
       "Survived  0.266100  0.108517  \n",
       "Pclass   -0.552893 -0.108502  \n",
       "Sex       0.182457  0.097129  \n",
       "Age       0.093143  0.012186  \n",
       "SibSp     0.139860  0.004021  \n",
       "Parch     0.206624 -0.014082  \n",
       "Fare      1.000000  0.176859  \n",
       "Embarked  0.176859  1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.corr()\n",
    "# here is a table that shows the correlation between each feature with the rest of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    1.000000\n",
       "Pclass     -0.356462\n",
       "Sex         0.536762\n",
       "Age        -0.082446\n",
       "SibSp      -0.015523\n",
       "Parch       0.095265\n",
       "Fare        0.266100\n",
       "Embarked    0.108517\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.corr()['Survived'] \n",
    "# we will hone on the correlation values between the target feature since that's what we are investigating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features removed out of 8 is 5 , leaving 3 selected features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pclass', 'Sex', 'Fare']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_corr=train.corr()['Survived'].abs() >= .2\n",
    "highly_correlated =all_corr[all_corr==True].index\n",
    "# Only the features that have a correlation value of 0.05 or above with the target will be selected. \n",
    "highly_correlated= highly_correlated.tolist() \n",
    "highly_correlated.remove('Survived')\n",
    "print('The number of features removed out of', all_corr.size, 'is', all_corr.size- len(highly_correlated),', leaving',len(highly_correlated),'selected features.')\n",
    "highly_correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Only the selected features in the list 'highly_correlated' will be used to make predictions for the 'Survived' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.7991431104107161\n",
      "Best Parameters: {'max_depth': 3}\n",
      "Average Time to Fit (s):0.005\n",
      "Average Time to Score (s):0.003\n"
     ]
    }
   ],
   "source": [
    "#Only colums with features that have a correlation value 0.2 or above as in the highly_correlated list \n",
    "#will be used to make predictions for the 'Survived' column.\n",
    "X_subsetted = X[highly_correlated]\n",
    "print(get_best_model_and_accuracy(d_tree, tree_params, X_subsetted, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hypothesis testing using KBest <a id=\"4.2\"></a>\n",
    "This is a method that involves calculating p-value to determine whether a hypothesis can be rejected or not.<br/> P-value stands for 'probablity value', it indicates how likely it is that a result occured by chance alone. <br/>\n",
    "SelectKBest scores the features against the target variable using a function (in this case f_classif but could be others) and then keeps the most significant features i.e the features that have the highest p-value. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "This models a statistical test known as ANOVA. <br/>\n",
    "ANNOVA: Analysis of variance is a collection of statistical models and their associated estimation procedures used to analyze the differences among group means in a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the best 4 features according to p-values of ANOVA test\n",
    "k_best = SelectKBest(f_classif, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the data and then tranform it.\n",
    "k_best.fit_transform(X, y)\n",
    "k_best"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "You can read more about fit_transform here https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>2.242852e-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>9.303620e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fare</td>\n",
       "      <td>5.256796e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Embarked</td>\n",
       "      <td>3.742742e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     column       p_value\n",
       "1       Sex  2.242852e-54\n",
       "0    Pclass  9.303620e-23\n",
       "5      Fare  5.256796e-13\n",
       "6  Embarked  3.742742e-03"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the p values of columns\n",
    "k_best.pvalues_\n",
    "# make a dataframe of features and p-values\n",
    "# sort that dataframe by p-value\n",
    "p_values = pd.DataFrame({'column': X.columns, 'p_value': k_best.pvalues_}).sort_values('p_value')\n",
    "# show the top 4 features\n",
    "p_values.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>2.242852e-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>9.303620e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fare</td>\n",
       "      <td>5.256796e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Embarked</td>\n",
       "      <td>3.742742e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     column       p_value\n",
       "1       Sex  2.242852e-54\n",
       "0    Pclass  9.303620e-23\n",
       "5      Fare  5.256796e-13\n",
       "6  Embarked  3.742742e-03"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features with a low p value\n",
    "p_values[p_values['p_value'] < .01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21], 'k_best__k': [1, 2, 3, 4, 5, 6, 'all']} \n",
      "\n",
      "Results for training set: \n",
      " Best Accuracy: 0.8034374076627598\n",
      "Best Parameters: {'classifier__max_depth': 5, 'k_best__k': 6}\n",
      "Average Time to Fit (s):0.008\n",
      "Average Time to Score (s):0.003 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_best = SelectKBest(f_classif)\n",
    "# setting the paramters\n",
    "tree_pipe_params = {'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]}\n",
    "# The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n",
    "select_k_pipe = Pipeline([('k_best', k_best),('classifier', d_tree)])\n",
    "select_k_best_pipe_params = deepcopy(tree_pipe_params)\n",
    "select_k_best_pipe_params.update({'k_best__k':[1,2,3,4,5,6] + ['all']})\n",
    "print(select_k_best_pipe_params,'\\n')\n",
    "\n",
    "print('Results for training set:','\\n',get_best_model_and_accuracy(select_k_pipe, select_k_best_pipe_params, X, y),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Check https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline to read about Pipeline function."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pros and Cons of Pearsn's and Hypothesis Testing: \n",
    "**Correlation coefficient like Pearson's: <br />**\n",
    "**Pros:** <br/>\n",
    "1- It gives an idea about how well the variables are related to each other.<br/>\n",
    "**Cons:** <br/>\n",
    "1- It assumes that there is always of linear relationship between the variables which might not be the case at all time.<br/>\n",
    "2- It does not determine causation between any two variables.\n",
    "<br />\n",
    "<br />\n",
    "**Hypthesis Testing:** <br />\n",
    "**Pros:** <br/>\n",
    "1- It aids in reaching a conclusion by examining a sample. <br />\n",
    "**Cons:** <br/>\n",
    "1- Results of significance tests are based on probabilities and as such cannot be expressed with full certainty. <br />\n",
    "2- Statistical inferences based on the significance tests cannot be said to be entirely correct evidences concerning the truth of the hypothesis.\n",
    "***\n",
    "It is worthy to note the correlation and  p-value are the most commoon statistical tests for identifying relationship between variables. \n",
    "\n",
    "**Difference between correlation and P value:**<br />\n",
    "\n",
    "Correlation is used to measure *how strong a relationship is between variables*. <br />\n",
    "Famous types of correlations: <br />\n",
    "1- Pearson <br />\n",
    "2- Kendall <br />\n",
    "3- Spearman <br />\n",
    "\n",
    "On the other hand, p-value measures *how well your data rejects the null hypothesis*, which claims that the two compared have no relationship. \n",
    "\n",
    "Both correlation and p-value give a measure about the relationship but correlation does not imply causation while p-value provides some support for causation as is present in the dataset. You can read more about this here https://lumina.com/causal-hypothesis-testing/#:~:text=The%20framework%20defines%20a%20p,is%20used%20in%20classic%20statistics.\n",
    "***\n",
    "**Bottom line:** <br />\n",
    "If you want to measure the statistical significance between two continuous variables, use correlation techniques like Pearson's. \n",
    "\n",
    "If you want to draw conclusions about the population using sample data, go for hypothesis testing. A hypothesis test basically helps us in making a decision about the population supported by sample data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2- Model-Based Selection <a id=\"5\"></a>\n",
    "Model selection is the process of choosing between different machine learning approaches or choosing between different hyperparameters or sets of features for the same machine learning approach. <br/>\n",
    "The two main machine learning models that we will use in this section for the purposes of feature selection are tree-based models and linear models.They both have a notion of feature ranking that are useful when subsetting feature sets.<br/>  In this dataset, we will use decision tree classifier for tree-based model and logistic regression and SVM for linear models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_test_ split is used to split the dataset into two pieces, so that the model can be trained and tested on different data. This is a better method for evaluating the model performance rather than testing it on the training data only. \n"
     ]
    }
   ],
   "source": [
    "# by default, 75% goes to the training set while 25% goes to the test set. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "print('train_test_ split is used to split the dataset into two pieces, so that the model can be trained and tested on different data. This is a better method for evaluating the model performance rather than testing it on the training data only. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Check this to have a better understanding of train_test_split https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "# i) Decision Trees <a id=\"5.1\"></a>\n",
    "A decision tree is a decision support tool that uses a tree-like model of decisions. Decision Trees provide an effective means for decision making because they consider all possible branches / scenarios as well as their outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.297360</td>\n",
       "      <td>Sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.267482</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.188768</td>\n",
       "      <td>Fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.143748</td>\n",
       "      <td>Pclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058308</td>\n",
       "      <td>SibSp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   importance feature\n",
       "1    0.297360     Sex\n",
       "2    0.267482     Age\n",
       "5    0.188768    Fare\n",
       "0    0.143748  Pclass\n",
       "3    0.058308   SibSp"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X, y)\n",
    "importances = pd.DataFrame({'importance': tree.feature_importances_,'feature':X.columns}).sort_values('importance', ascending=False)\n",
    "importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "SelectfromModel is a skikit wrapper that captures the top k most importance features by considering a machine learning interal metric for feature importance. SelectFromModel is similar to SelectKBest as it picks the top k most important features. However, it measures the importance of a feature based on a model's internal metric for feature importance rather than the p-value. You can read more about SelectFromModel here https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html?highlight=selectfrom%20mode#sklearn.feature_selection.SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "select_from_model = SelectFromModel(DecisionTreeClassifier(),threshold=.05)\n",
    "selected_X = select_from_model.fit_transform(X, y)\n",
    "# this shows the features that are selected by the model. \n",
    "selected_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21], 'select__threshold': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 'mean', 'median', '2.*mean'], 'select__estimator__max_depth': [None, 1, 3, 5, 7]} \n",
      "\n",
      "Results for training set: \n",
      " Best Accuracy: 0.8202080761770411\n",
      "Best Parameters: {'classifier__max_depth': 3, 'select__estimator__max_depth': None, 'select__threshold': 0.01}\n",
      "Average Time to Fit (s):0.009\n",
      "Average Time to Score (s):0.003 \n",
      "\n",
      "Results for test set: \n",
      " Best Accuracy: 0.7696825396825396\n",
      "Best Parameters: {'classifier__max_depth': None, 'select__estimator__max_depth': 1, 'select__threshold': 0.01}\n",
      "Average Time to Fit (s):0.009\n",
      "Average Time to Score (s):0.003\n"
     ]
    }
   ],
   "source": [
    "# create a SelectFromModel that is tuned by a DecisionTreeClassifier\n",
    "select = SelectFromModel(DecisionTreeClassifier())\n",
    "select_from_pipe = Pipeline([('select', select),('classifier', d_tree)])\n",
    "select_from_pipe_params = deepcopy(tree_pipe_params)\n",
    "select_from_pipe_params.update({'select__threshold': [.01, .05, .1, .2, .25, .3, .4, .5, .6, \"mean\",\"median\", \"2.*mean\"],'select__estimator__max_depth': [None, 1, 3, 5, 7]})\n",
    "print(select_from_pipe_params,'\\n')\n",
    "print('Results for training set:','\\n', get_best_model_and_accuracy(select_from_pipe,select_from_pipe_params,X_train, y_train), '\\n')\n",
    "print('Results for test set:','\\n',get_best_model_and_accuracy(select_from_pipe,select_from_pipe_params,X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_from_pipe.set_params(**{'select__threshold': 0.01,'select__estimator__max_depth': None,'classifier__max_depth': 3})\n",
    "# fit our pipeline to our data\n",
    "select_from_pipe.steps[0][1].fit(X, y)\n",
    "# this presents the name of the featurs that are selected from the model.\n",
    "X.columns[select_from_pipe.steps[0][1].get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We could continue onward by trying several other tree-based models, such as RandomForest, ExtraTreesClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Decision trees pros and cons: \n",
    "\n",
    "**Pros:** <br/>\n",
    "1- Compared to other algorithms, decision trees require **less pre-processing**.Decision trees are definitely more robust to Outliers and missing values than regression techniques. <br/>\n",
    "2- **No normalization** required. <br/>\n",
    "3- **No scaling** required. <br/>\n",
    "4- **Not necessary to deal with missing values**. Missing values data does not affect the process of building decision tree. This is because they work on segmentation of population and treat all missing values as a different class itself.<br/>\n",
    "5- Desicion trees are **easy to understand**.Decision trees require no complex formulas. They consider all of the decision alternatives for quick comparisons in a format that is comprehensible.<br/>\n",
    "\n",
    "**Cons:**<br/>\n",
    "1- Mathematical calculation of decsion tree requires **more memory and time**.<br/>\n",
    "2- A small change in the data can result in large change in the tree structure, thus this can have a **large effect on the tree model sensitivity**. \n",
    "<br/>\n",
    "3-**Higher space and time complexity** for a decision tree. \n",
    "***\n",
    "As a side note, a single decision tree is often a weak learner so random forect is usually used instead for better prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ii) Logistic Regression <a id=\"5.2\"></a>\n",
    "A linear regression model predicts the target as a weighted sum of the feature inputs. \n",
    "Logistic Regression measures the relationship between the dependent variable (our label, what we want to predict) and the one or more independent variables (our features), by estimating probabilities using itâ€™s underlying logistic function.<br/>\n",
    "Linear models work by placing coefficients next to features that tells how much it affects the response when the feature is changed. <br/>\n",
    "You can read more about it's technicality here https://christophm.github.io/interpretable-ml-book/limo.html#limo."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# When do we use logistic Regression? \n",
    "**You should think about using logistic regression when your Y variable takes on only two values (e.g when you are facing a classification problem)**\n",
    "\n",
    "# When is using any type of regression not suitable?\n",
    "The two most common kinds of issues are (1) when your data contain major violations of regression assumptions and (2) when you don't have enough data (or of the right kinds).\n",
    "\n",
    "Core assumptions behind regression include\n",
    "\n",
    "- That there is in fact a relationship between the outcome variable and the predictor variables.\n",
    "\n",
    "- That observations are independent.\n",
    "\n",
    "- That the residuals are **normally distributed** and **independent** of the values of variables in the model.\n",
    "\n",
    "- That each predictor variable is not a linear combination of any others and is not extremely correlated with any others.\n",
    "\n",
    "- Additional assumptions depend on the nature of your dependent variable; for example whether it is measured on a continuous scale or is categorical yes/no etc. The form of regression you use (linear, logistic, etc.) must match the type of data.\n",
    "\n",
    "Not having enough data means having very few cases at all or having large amounts of missing values for the variables you want to analyze. If you don't have enough observations, your model either will not be able to run or else the estimates could be so imprecise (with large standard errors) that they aren't useful.\n",
    "***\n",
    "source: https://www.answers.com/Q/When_regression_is_not_applicable?#slide=1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "In linear models, regularization is a method for imposing additional constraints to a\n",
    "learning model, where the goal is to prevent overfitting and improve the generalization of\n",
    "the data. l1 and l2 are regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21], 'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median', '2.*mean'], 'select__estimator__penalty': ['l1', 'l2']}\n",
      "Results for training set: \n",
      " Best Accuracy: 0.7996825956621407\n",
      "Best Parameters: {'classifier__max_depth': 5, 'select__estimator__penalty': 'l2', 'select__threshold': 0.01}\n",
      "Average Time to Fit (s):0.027\n",
      "Average Time to Score (s):0.002 \n",
      "\n",
      "Results for test set: \n",
      " Best Accuracy: 0.7752380952380953\n",
      "Best Parameters: {'classifier__max_depth': 3, 'select__estimator__penalty': 'l2', 'select__threshold': 0.05}\n",
      "Average Time to Fit (s):0.02\n",
      "Average Time to Score (s):0.001\n"
     ]
    }
   ],
   "source": [
    "logistic_selector = SelectFromModel(LogisticRegression(max_iter = 1500))\n",
    "regularization_pipe = Pipeline([('select', logistic_selector),('classifier', tree)])\n",
    "regularization_pipe_params = deepcopy(tree_pipe_params)\n",
    "regularization_pipe_params.update({'select__threshold': [.01, .05, .1, \"mean\", \"median\", \"2.*mean\"],'select__estimator__penalty': ['l1', 'l2'],})\n",
    "# l1 and l2 are regularization methods.\n",
    "\n",
    "print(regularization_pipe_params) \n",
    "print('Results for training set:','\\n',get_best_model_and_accuracy(regularization_pipe,regularization_pipe_params,X_train, y_train),'\\n')\n",
    "print('Results for test set:','\\n',get_best_model_and_accuracy(regularization_pipe,regularization_pipe_params,X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularization_pipe.set_params(**{'select__threshold': 0.01,'classifier__max_depth': 5,'select__estimator__penalty': 'l2'})\n",
    "# fit our pipeline to our data\n",
    "regularization_pipe.steps[0][1].fit(X, y)\n",
    "# list the columns  selected by calling the get_support() method from SelectFromModel\n",
    "X.columns[regularization_pipe.steps[0][1].get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pros and Cons of Logistic Regression:\n",
    "**Pros:** <br/>\n",
    "1- Logistic regression can be a good (and effective) choice, provided that your dataset is fit for it.[Click here for explanation](http://) <br/>\n",
    "2 Logistic regression is less prone to over-fitting but it can overfit in high dimensional datasets. You should consider Regularization (L1 and L2) techniques to avoid over-fitting in these scenarios. Regularization is a method for imposing additional constraints to a learning model.<br/>\n",
    "3- In addition to giving a measure of how relevant a coefficient size, logistic regression also provides information about the predictor's association ( positive or negative). \n",
    "<br/><br/>\n",
    "**Cons:** <br/>\n",
    "1- can only be used to predict discrete functions.<br/>\n",
    "2- should not be used when the number of observations is less than the number of features. <br/>\n",
    "3- assumes that there is linearity between dependent and independent variables which is rarely the case as data is usually a unorganized. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# iii) LinearSVC <a id=\"5.3\"></a>\n",
    "\n",
    "SVC is a linear model that uses linear supports to seperate classes in euclidean space. This model can only work for binary classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__max_depth': [None, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21], 'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median', '2.*mean'], 'select__estimator__penalty': ['l1', 'l2'], 'select__estimator__loss': ['squared_hinge', 'hinge'], 'select__estimator__dual': [True, False]} \n",
      "\n",
      "Results for training set: \n",
      " Best Accuracy: 0.7996825956621407\n",
      "Best Parameters: {'classifier__max_depth': 5, 'select__estimator__dual': False, 'select__estimator__loss': 'squared_hinge', 'select__estimator__penalty': 'l1', 'select__threshold': 0.01}\n",
      "Average Time to Fit (s):0.328\n",
      "Average Time to Score (s):0.001 \n",
      "\n",
      "Results for test set: \n",
      " Best Accuracy: 0.7696825396825396\n",
      "Best Parameters: {'classifier__max_depth': None, 'select__estimator__dual': True, 'select__estimator__loss': 'hinge', 'select__estimator__penalty': 'l2', 'select__threshold': '2.*mean'}\n",
      "Average Time to Fit (s):0.109\n",
      "Average Time to Score (s):0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svc_selector = SelectFromModel(LinearSVC(max_iter=100000,dual=False))\n",
    "svc_pipe = Pipeline([('select', svc_selector),('classifier', tree)])\n",
    "svc_pipe_params = deepcopy(tree_pipe_params)\n",
    "svc_pipe_params.update({'select__threshold': [.01, .05, .1, \"mean\", \"median\", \"2.*mean\"],'select__estimator__penalty': ['l1', 'l2'],'select__estimator__loss': ['squared_hinge', 'hinge'],'select__estimator__dual': [True, False]})\n",
    "print(svc_pipe_params,'\\n') # 'select__estimator__loss': ['squared_hinge','hinge'], 'select__threshold': [0.01, 0.05, 0.1, 'mean', 'median','2.*mean'], 'select__estimator__penalty': ['l1', 'l2'],'classifier__max_depth': [1, 3, 5, 7], 'select__estimator__dual': [True,False]}\n",
    "print('Results for training set:','\\n',get_best_model_and_accuracy(svc_pipe,svc_pipe_params,X_train, y_train),'\\n')\n",
    "print('Results for test set:','\\n',get_best_model_and_accuracy(svc_pipe,svc_pipe_params,X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked'], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_pipe.set_params(**{'select__estimator__loss': 'squared_hinge','select__threshold': 0.01,'select__estimator__penalty': 'l1','classifier__max_depth': 5,'select__estimator__dual': False})\n",
    "# fit our pipeline to our data\n",
    "svc_pipe.steps[0][1].fit(X, y)\n",
    "# list the columns that the SVC selected by calling the get_support() method from SelectFromModel\n",
    "X.columns[svc_pipe.steps[0][1].get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "The SVM module (SVC, NuSVC, etc) is a **wrapper** around the libsvm library and supports different kernels while LinearSVC is based on liblinear and only supports a linear kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Note: <br/>\n",
    "We have a come across the term 'wrapper' several times in this tutorial. There are three classes for feature selection: filter, wrapper, and embedded methods. Wrapper methods evaluate the importance of features based on the classifier performance. Filter methods measure evaluates the features via univariate statistics instead of cross-validation performance.Finally, embedded methods, are quite similar to wrapper methods, however, the difference is that an intrinsic model building metric is used during learning. Please refer back to this to learn more https://sebastianraschka.com/faq/docs/feature_sele_categories.html."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pros and cons of SVC: \n",
    "**Pros:** <br/>\n",
    "1- Effective in high dimensional spaces. <br/>\n",
    "2- Memory efficient because it uses a subset of training points in the decision vectore ( support vectors).<br/>\n",
    "\n",
    "**Cons:**<br/>\n",
    "1- Similar to linear regression, SVC should not be used when the number of observations is less than the number of features ( this is known as overfitting). <br/>\n",
    "2- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#  Now, having gone through the three model-based selection techniques, which one do you think is the most efficient in this dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tips on when to use each technique: \n",
    "If your features are mostly categorical, you should start by trying to implement a SelectKBest with a Chi2 ranker or a tree-based model selector ( like decision tree). <br/>\n",
    "If your features are largely quantitative, using linear models as model-based selectors and relying on correlations tends to yield greater results.<br/>\n",
    "If you are solving a binary classification problem, using a Support Vector Classification model along with a SelectFromModel selector will probably fit nicely, as the SVC tries to find coefficients to optimize for binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# References: <a id=\"6\"></a>\n",
    "* Introduction to machine learning by Andreas C. MÃ¼ller and Sarah Guido\n",
    "* Feature Engineering made easy by Sinan Ozdemir and Divya Susarla\n",
    "* https://christophm.github.io/interpretable-ml-book/limo.html#limo\n",
    "* https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n",
    "* https://dataschool.com/fundamentals-of-analysis/correlation-and-p-value/#:~:text=The%20two%20most%20commonly%20used,an%20experiment%20is%20statistically%20significant.\n",
    "* https://lumina.com/causal-hypothesis-testing/#:~:text=The%20framework%20defines%20a%20p,is%20used%20in%20classic%20statistics.\n",
    "* https://www.coursehero.com/file/p4jtjlo/The-disadvantages-of-the-Pearson-r-correlation-method-are-It-assumes-that-there/#:~:text=The%20disadvantages%20of%20the%20Pearson%20r%20correlation%20method%20are%3B%E2%9D%96,does%20not%20necessarily%20mean%20very\n",
    "* https://www.wisdomjobs.com/e-university/research-methodology-tutorial-355/limitations-of-the-tests-of-hypotheses-11539.html\n",
    "* And, ofcourse Qoura and stackOverflow. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
