{"cells":[{"metadata":{},"cell_type":"markdown","source":"In the tutorial we are required to make predictions for the 'Survived' Column in the titanic dataset.\n"},{"metadata":{},"cell_type":"markdown","source":"Table of Contents: \n1. Data Preparation\n     - [Data Exploration & Cleaning ](#0)\n2. Data Anaysis\n      - [Feature Construction](#1)\n      - [Feature selection](#2)\n      - [Model Selection ](#3)\n      \n  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train= pd.read_csv('../input/titanic/train.csv',index_col='PassengerId')\ntest = pd.read_csv('../input/titanic/test.csv', index_col = \"PassengerId\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import randint\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import ExtraTreesClassifier\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration and Cleaning<a id=\"0\"></a>"},{"metadata":{},"cell_type":"markdown","source":"First, we will explore the size of data that we have. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of traning examples(data points) = %i \" % train.shape[0])\nprint(\"The number of features = %i \" % train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nData cleaning is the process of ensuring that your data is correct, consistent and usable. This improves the quality of the training data for analytics and enables accurate decision-making.<br/>\n\nFor data cleaning, we will focus on three points: \n* Non-numerical data \n* Missing values\n* Outliers \n"},{"metadata":{},"cell_type":"markdown","source":"First, let's check the size of null values we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will to examine the data types of our features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results, we can see that we have 5 columns of type 'Object'. We need to decide on how we will use these features. The main purpose here is to prepare data that can be used in machine learning models and for that, we need our data to be numerical. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns[train.dtypes==object]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Sex\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Embarked\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that Sex and Embarked are nominal features, and they contain a few number of unique values. Some ML libraries do not take categorical variables as input. Thus, we will convert them into numerical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanup_nums = { \"Embarked\": {\"S\": 0, \"C\": 1, \"Q\": 2 },\"Sex\": {\"male\": 0, \"female\": 1}}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.replace(cleanup_nums, inplace=True)\ntest.replace(cleanup_nums, inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's take a look at the Cabin and Ticket features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Cabin'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Ticket'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the are too many unique values for Cabin and Ticket features; it might be reasonable to remove them, but we will try to extract to some information from them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's make boxplots to visualise outliers in the continuous variables \n# Age and Fare\n \nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n \nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we plot the distributions to find out if they are Gaussian or skewed.\nDepending on the distribution, we will use the normal assumption or the interquantile range to find outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first we plot the distributions to find out if they are Gaussian or skewed.\n# Depending on the distribution, we will use the normal assumption or the interquantile\n# range to find outliers\n \nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.Age.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Age')\n \nplt.subplot(1, 2, 2)\nfig = train.Fare.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Fare')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age has a normal distribution while the fare feature has skewed distribution. For the age feature, we will use the Gaussian assumption , and the interquantile range for Fare."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find outliers\n# Age\nUpper_boundary = train.Age.mean() + 3* train.Age.std()\nLower_boundary = train.Age.mean() - 3* train.Age.std()\nprint('Age outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_boundary, upperboundary=Upper_boundary))\n \n# Fare\nIQR = train.Fare.quantile(0.75) - train.Fare.quantile(0.25)\nLower_fence = train.Fare.quantile(0.25) - (IQR * 3)\nUpper_fence = train.Fare.quantile(0.75) + (IQR * 3)\nprint('Fare outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will replace outliers with reasonable values based on the above calculations."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age'] = np.where(train['Age']>73, 73, train['Age'])\ntest['Age'] = np.where(test['Age']>73, 73, test['Age'])\ntrain['Age'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Fare'] = np.where(train['Fare']>100, 100, train['Fare'])\ntest['Fare'] = np.where(test['Fare']>100, 100, test['Fare'])\ntrain['Fare'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I referred to this great kernel for outlier detection https://www.kaggle.com/anuragnegi/feature-engineering-outliers-handling-ensembling."},{"metadata":{},"cell_type":"markdown","source":"Now we will take a look at the correlation matrix to get a quick insight about the relationships between features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorr = train.corr()\nf, ax = plt.subplots(figsize=(20, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr,linewidths=.5, annot= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Construction <a id=\"1\"></a>\nFeature construction is a process which builds intermediate features from the original descriptors in a dataset. The aim is to build more efficient features for a machine learning task."},{"metadata":{},"cell_type":"markdown","source":"From the 'Name' feature, we can astract other important features such as the family name to identify members of the same family. It is likely that members of the same family withh have the same family name."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Family_name']=train['Name'].str.split(', ').str[0]\ntest['Family_name']=test['Name'].str.split(', ').str[0]\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, we can also derive the marital status of each member from the Name feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Title']=train['Name'].str.split(', ').str[1].str.split('.').str[0]\ntest['Title']=test['Name'].str.split(', ').str[1].str.split('.').str[0]\ntrain['Title'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Title'] =train['Title'].replace(['Ms','Mlle'], 'Miss')\ntrain['Title'] = train['Title'].replace(['Mme','Dona','the Countess','Lady'], 'Mrs')\ntrain['Title'] =train['Title'].replace(['Rev','Mlle','Jonkheer','Dr','Capt','Don','Col','Major','Sir'], 'Mr')\n\ntest['Title'] =test['Title'].replace(['Ms','Mlle'], 'Miss')\ntest['Title'] = test['Title'].replace(['Mme','Dona','the Countess','Lady'], 'Mrs')\ntest['Title'] =test['Title'].replace(['Rev','Mlle','Jonkheer','Dr','Capt','Don','Col','Major','Sir'], 'Mr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncleanup_nums = { \"Title\": {\"Mr\": 0, \"Mrs\": 1, \"Miss\": 2, \"Master\": 3 } }\ntrain.replace(cleanup_nums, inplace=True)\ntest.replace(cleanup_nums, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can divide 'Age' into categories and see whether this will be better than the original 'Age' feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age'].fillna((train['Age'].mean()), inplace=True) # I will fill the columns that do with nan values with mean age number \ntest['Age'].fillna((test['Age'].mean()), inplace=True)\n\nbins = [0, 2, 18, 35, 65, np.inf]\nnames = ['<2', '2-18', '18-35', '35-65', '65+']\n\ntrain['AgeRange'] = pd.cut(train['Age'], bins, labels=names)\ntest['AgeRange'] = pd.cut(test['Age'], bins, labels=names)\n\nNumberedAgeCategories = {'<2':0 , '2-18':1, '18-35':2, '35-65':3, '65+':4}\ntrain['AgeRange']=train['AgeRange'].map(NumberedAgeCategories)  \ntrain['AgeRange']=pd.to_numeric(train['AgeRange'])\ntest['AgeRange']=test['AgeRange'].map(NumberedAgeCategories)  \ntest['AgeRange']=pd.to_numeric(test['AgeRange'])\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since SibSp include information about the number of siblings and spouses altogether and Parch includes information about the number of nannies, we can extract the family size from this info. Another option would have been to use the family name to identify member of the same family but since the resemblance of family name might be a coincidence we should avoid using this feature for more accurate results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['FamilySize']= train['SibSp']+train['Parch']+1\ntest['FamilySize']= test['SibSp']+test['Parch']+1\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we can work out some meaningful interpretation from the Cabin column.<br/>\nDespite the fact that this feature contains too many unique values and null values, we can still see that this feature carries useful information like the deck group and the room number ( One letter followed by numbers). <br/>\nWe can use this to check whether there is any relation between the deck and the fare amount or the Pclass of a passenger. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_only = train[[\"Cabin\"]].copy()\ncabin_only[\"Cabin_Data\"] = cabin_only[\"Cabin\"].isnull().apply(lambda x: not x) # extract rows that do not contain null Cabin data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_only[\"Deck\"] = cabin_only[\"Cabin\"].str.slice(0,1)\ncabin_only[\"Room\"] = cabin_only[\"Cabin\"].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\ncabin_only[cabin_only[\"Cabin_Data\"]]\ncabin_only","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will deal the the null values in the Cabin column."},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_only.drop([\"Cabin\", \"Cabin_Data\"], axis=1, inplace=True, errors=\"ignore\")\ncabin_only[\"Deck\"] = cabin_only[\"Deck\"].fillna(\"N\") # assign 'N' for the deck name of the null Cabin value. \ncabin_only[\"Room\"] = cabin_only[\"Room\"].fillna(cabin_only[\"Room\"].mean()) # use mean to fill null Room values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_only","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we need to make sure our values are numerical so we will represent the column 'Deck' in a different way using pandas dummies. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_only=cabin_only.join(pd.get_dummies(cabin_only['Deck'], prefix='Deck'))\ncabin_only=cabin_only.drop(['Deck'], axis=1)\ncabin_only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.concat([train,cabin_only],axis=1)\ntrain.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I will just repeat the same process for test data. You can overlook this part. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cabin_only_test = test[[\"Cabin\"]].copy()\ncabin_only_test[\"Cabin_Data\"] = cabin_only_test[\"Cabin\"].isnull().apply(lambda x: not x) # extract rows that do not contain null Cabin data.\ncabin_only_test[\"Deck\"] = cabin_only_test[\"Cabin\"].str.slice(0,1)\ncabin_only_test[\"Room\"] = cabin_only_test[\"Cabin\"].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\ncabin_only_test[cabin_only_test[\"Cabin_Data\"]]\ncabin_only_test.drop([\"Cabin\", \"Cabin_Data\"], axis=1, inplace=True, errors=\"ignore\")\ncabin_only_test[\"Deck\"] = cabin_only_test[\"Deck\"].fillna(\"N\") # assign 'N' for the deck name of the null Cabin value. \ncabin_only_test[\"Room\"] = cabin_only_test[\"Room\"].fillna(cabin_only_test[\"Room\"].mean()) # use mean to fill null Room values.\ncabin_only_test=cabin_only_test.join(pd.get_dummies(cabin_only_test['Deck'], prefix='Deck'))\ncabin_only_test=cabin_only_test.drop(['Deck'], axis=1)\ntest=pd.concat([test,cabin_only_test],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarily, for the ticket feature, there is a pattern in the format which is letters followed by words; so we will extract this data to be able to see if it has an effect on the y_target('Survived' column). "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# extract numbers from the ticket\ntrain['Ticket_numerical'] = train.Ticket.apply(lambda s: s.split()[-1])\ntrain['Ticket_numerical'] = np.where(train.Ticket_numerical.str.isdigit(), train.Ticket_numerical, np.nan)\ntrain['Ticket_numerical'] = train['Ticket_numerical'].astype('float')\ntrain[\"Ticket_numerical\"] = train[\"Ticket_numerical\"].fillna(0) # some tickets have string values only, so we will assign a 0 for their ticket_numerical.\n\n\ntest['Ticket_numerical'] = test.Ticket.apply(lambda s: s.split()[-1])\ntest['Ticket_numerical'] = np.where(test.Ticket_numerical.str.isdigit(), test.Ticket_numerical, np.nan)\ntest['Ticket_numerical'] = test['Ticket_numerical'].astype('float')\ntest[\"Ticket_numerical\"] = test[\"Ticket_numerical\"].fillna(0) \n\n# extract the first part of ticket as category\ntrain['Ticket_categorical'] = train.Ticket.apply(lambda s: s.split()[0])\ntrain['Ticket_categorical'] = np.where(train.Ticket_categorical.str.isdigit(), np.nan, train.Ticket_categorical)\ntrain[\"Ticket_categorical\"] = train[\"Ticket_categorical\"].fillna(\"NONE\") # some tickets have digit values only, so we will assign 'NONE' for their ticket_categorical.\ntrain['Ticket_numerical'].tolist()\n \ntest['Ticket_categorical'] = test.Ticket.apply(lambda s: s.split()[0])\ntest['Ticket_categorical'] = np.where(test.Ticket_categorical.str.isdigit(), np.nan, test.Ticket_categorical)\ntest[\"Ticket_categorical\"] = test[\"Ticket_categorical\"].fillna(\"NONE\") # some tickets have digit values only, so we will assign 'NONE' for their ticket_categorical.\ntest['Ticket_numerical'].tolist()\n\ntrain[['Ticket', 'Ticket_numerical', 'Ticket_categorical']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After adding all these new features, we need to check whether we have null values and deal with them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is reasonable to remove the Cabin column now that we have extracted two new features from it : 'Deck' and 'Room'. <br/>\nFor the rest of the null values, we can drop the rows with null values since they are few and this will not affect our machine learning model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fare in the test dataset contains one null value, I will replace it by the median \ntrain.Fare.fillna(train.Fare.median(), inplace=True)\ntest.Fare.fillna(train.Fare.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train= train.drop(['Cabin'], axis=1)\ntest= test.drop(['Cabin'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection <a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Feature Selection is the process of selecting a subset of relevant features for use in model construction."},{"metadata":{},"cell_type":"markdown","source":"For this dataset, we will examine the effect of features on 5 different models: </br>\n\n1. Decision Trees:A Decision Tree is a Flow Chart, and can help you make decisions based on previous experience. \n2. Random Forests:Random Forest is essentially a collection of Decision Trees.\n3. Logistic Regression: Logistic Regression is a type of Generalized Linear Models. Logistic regression models the probabilities for classification problems with two possible outcomes.\n4. XGBoost: XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n5. Extra Trees: Extra Trees is an ensemble machine learning algorithm that combines the predictions from many decision trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoder can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\nlabel_encoder = LabelEncoder()\nfor col in train.columns[train.dtypes == \"object\"]:\n    train[col] = label_encoder.fit_transform(train[col].astype('str'))\n\nfor col in test.columns[test.dtypes == \"object\"]:\n    test[col] = label_encoder.fit_transform(test[col].astype('str'))\n\n# drop rows with null values    \ntrain.dropna(inplace=True)\n\nX = train.drop('Survived', axis=1)\n\n# create our response variable\ny = train['Survived']\n\n\n#train_test_ split is used to split the dataset into two pieces, so that the model can be trained and tested on different data.\n#This is a better method for evaluating the model performance rather than testing it on the training data only. \nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def featureSelection(label):    \n    clf = DecisionTreeClassifier(random_state=0)\n    if(label=='Decision Tree'):\n        clf = DecisionTreeClassifier(random_state=0)\n    if(label=='Random Forest'):\n        clf = RandomForestClassifier(random_state=0)\n    if(label=='XGBoost'):\n        clf = XGBClassifier(random_state=0)  \n    if(label=='Extra Trees'):\n        clf = ExtraTreesClassifier(random_state=0)  \n        \n    clf= clf.fit(X_train, y_train)\n    \n    arr= dict(zip(X_train.columns, clf.feature_importances_)) ## this is used to write the feature name next to the probability\n    data= pd.DataFrame.from_dict(arr,orient='index', columns=['importance'])\n    return data.sort_values(['importance'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will display the features with their corresponding importance values based on each model."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(20,10)) # one row, three columns\nr=featureSelection(\"Decision Tree\")\nv=featureSelection(\"Random Forest\")\ns=featureSelection(\"XGBoost\")\nt=featureSelection(\"Extra Trees\")\nr.plot.bar(y=\"importance\", rot=70, title=\"Decision Tree Features with their corresponding importance values\",ax=ax1)\nv.plot.bar(y=\"importance\", rot=70, title=\"Random Forest Features with their corresponding importance values\", ax=ax2)\ns.plot.bar(y=\"importance\", rot=70, title=\"XGBoost Features with their corresponding importance values\", ax=ax3)\nt.plot.bar(y=\"importance\", rot=70, title=\"Extra Trees Features with their corresponding importance values\", ax=ax4)\nplt.tight_layout() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlogit_model = LogisticRegression(max_iter=10000)\nlogit_model.fit(X_train, y_train)\n \nimportance = pd.Series(np.abs(logit_model.coef_.ravel()))\nimportance.index = X_train.columns\nimportance.sort_values(inplace=True, ascending=False)\nimportance.plot.bar(figsize=(12,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that all the columns with prefix 'Deck' and the AgeRange seem to have little importance in all 4 classifiers. We will check under the 'Model Selection' part whether removing them will improve the accuarcy of the classifier. ( This is an A/B testing method)"},{"metadata":{},"cell_type":"markdown","source":"# Model Selection<a id=\"3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Model selection is the process of selecting one final machine learning model from among a collection of machine learning models for a training dataset. "},{"metadata":{},"cell_type":"markdown","source":"Grid search is used to tune hyperparameters to improve model performance. You can read more about it here https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_model_and_accuracy(model, params, X, y):\n    grid_clf_auc = GridSearchCV(model,param_grid=params,error_score=0.,scoring = 'roc_auc')\n    grid_clf_auc.fit(X, y) # fit the model and parameters\n    print('Grid best parameter (max. AUC): ', grid_clf_auc.best_params_)\n    print('Grid best score (AUC): ', grid_clf_auc.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will the scoring of each model. "},{"metadata":{},"cell_type":"markdown","source":"Before deleting the columns with prefix 'Deck' and the AgeGroup column, I will check the accuracy score with and without these columns in 3 classifiers as a test to make sure removing them is beneficial."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt=DecisionTreeClassifier(random_state=0)\nparam_grid = {\"max_depth\": [3,7,10,50,100],\n              \"min_samples_leaf\": [1,2,3,4,5,6,7,8,9],\n              \"criterion\": [\"gini\", \"entropy\"]}  \nprint(\"Decision Tree train:\")\nget_best_model_and_accuracy(dt, param_grid, X_train, y_train)\nprint(\"Decision Tree test:\")\nget_best_model_and_accuracy(dt, param_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\nLR=LogisticRegression(max_iter=100000,random_state=0)\nprint(\"Logistic Regression train:\")\nget_best_model_and_accuracy(LR, param_grid, X_train, y_train)\nprint(\"Logistic Regression test:\")\nget_best_model_and_accuracy(LR, param_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n}\nrf=RandomForestClassifier(random_state=0)\nprint(\"Random Forest train:\")\nget_best_model_and_accuracy(rf, param_grid, X_train, y_train)\nprint(\"Random Forest test:\")\nget_best_model_and_accuracy(rf, param_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I will repeat the same process but with some columns(those with the predix 'Deck' and AgeRange column) being removed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train[train.columns.drop(list(train.filter(regex='Deck')))]\ntrain= train.drop(['AgeRange'], axis=1)\ntest=test[test.columns.drop(list(test.filter(regex='Deck')))]\ntest= test.drop(['AgeRange'], axis=1)\nX = train.drop('Survived', axis=1)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt=DecisionTreeClassifier(random_state=0)\nparam_grid = {\"max_depth\": [3,7,10,50,100],\n              \"min_samples_leaf\": [1,2,3,4,5,6,7,8,9],\n              \"criterion\": [\"gini\", \"entropy\"]}  \nprint(\"Decision Tree train:\")\nget_best_model_and_accuracy(dt, param_grid, X_train, y_train)\nprint(\"Decision Tree test:\")\nget_best_model_and_accuracy(dt, param_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\nLR=LogisticRegression(max_iter=100000,random_state=0)\nprint(\"Logistic Regression train:\")\nget_best_model_and_accuracy(LR, param_grid, X_train, y_train)\nprint(\"Logistic Regression test:\")\nget_best_model_and_accuracy(LR, param_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n}\nrf=RandomForestClassifier(random_state=0)\nprint(\"Random Forest train:\")\nget_best_model_and_accuracy(rf, param_grid, X_train, y_train)\nprint(\"Random Forest test:\")\nget_best_model_and_accuracy(rf, param_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the difference is very small, removing them still results in better results. "},{"metadata":{},"cell_type":"markdown","source":"Now I will find the accuracy score for the rest of the classifiers after removing these features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'max_depth': [3, 4, 5]\n        }\nXGB= XGBClassifier(random_state=0)\nprint(\"XGBoost train:\")\nget_best_model_and_accuracy(XGB, param_grid, X_train, y_train)\nprint(\"XGBoost test:\")\nget_best_model_and_accuracy(XGB, param_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" param_grid={\n    \n     'n_estimators':[100,500,1000], 'max_depth':[5,6,9], 'min_samples_split':[5,6,9], 'min_samples_leaf':[4,5,6,9]\n       \n    }\nET= ExtraTreesClassifier(random_state=0) \nprint(\"ExtraTrees train:\")\nget_best_model_and_accuracy(ET, param_grid, X_train, y_train)\nprint(\"ExtraTrees test:\")\nget_best_model_and_accuracy(ET, param_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The difference between train and test scores are small which is a good indication that there is no over-fitting. Finally, it would be reasonable to choose the model with the hight score for X_Test dataset, which in this case would be Random Forest. "},{"metadata":{},"cell_type":"markdown","source":"We will use the best paramaters for random forest classifier that were identified by GridSearchCV in the code above. "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf= RandomForestClassifier(random_state=0,max_depth= 7, max_features= 'sqrt', n_estimators= 500)\nrf.fit(X_train,y_train)\npredictions = rf.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the Feature Selection graphs for random forest ( the selected model ), features of Sex and Title had the greatest influence. In order to determine the most important category within each feature, I will use visuals: "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='Sex', y='Survived', data=train, estimator=lambda x: sum(x==0)*100.0/len(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As noted in the data analysis part, 0 refers to male. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='Title', y='Survived', data=train, estimator=lambda x: sum(x==0)*100.0/len(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0 in this plot refers to men with the title \"Mr\". "},{"metadata":{},"cell_type":"markdown","source":"Hence, we can conlude the men of age 18 and above had the largest chance for survival. "},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df = pd.DataFrame()\nresults_df[\"PassengerId\"] = test.index\nresults_df[\"Survived\"] = predictions\nresults_df.to_csv(\"my_submissions\", index=False)\nresults_df.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}